{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"2","provenance":[],"private_outputs":true,"collapsed_sections":[],"mount_file_id":"1FEvo81dGwkwPWooyU7_DuPRoKUaVGgiz","authorship_tag":"ABX9TyOaskKrGS6Xnhtwk5Riyilg"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"id":"tdjD0jYXhJPP","colab_type":"code","colab":{}},"source":["#!git clone https://github.com/titu1994/DenseNet.git\n","#!pip install PyQt5\n","#%cd sample_data/\n","#%cd ..\n","#!ls"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"aHwMyzP4v86S","colab_type":"code","colab":{}},"source":["!curl https://faradars.org/courses/fvcvl9510-earthquake-engineering  --output fara2.htm"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"XPyEG9OFd5Kc","colab_type":"code","colab":{}},"source":["#  \n","import requests\n","def Downld(urll):\n","  url=urll\n","  resp = requests.get(url)\n","  cont = resp.content\n","  file_name=url.split(\"/\")[-1]\n","  if \"?\" in file_name:\n","    file_name = file_name.split(\"?\")[0]\n","  with open(file_name,\"wb\") as file1:\n","    file1.write(cont)\n","while True:\n","  try:\n","    Downld(str(input(\"url: \")))\n","    print(\"DONE\")\n","  except:\n","    print(\"ERROR\")\n","    break"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"DSEZb6VxzU13","colab_type":"code","colab":{}},"source":["# !git clone https://github.com/iamleot/transferwee.git\n","def up_to_wetransfer(filee):\n","  # !git clone https://github.com/iamleot/transferwee.git\n","  import os\n","  from transferwee import transferwee\n","  print(transferwee.upload([filee])) # ,recipients=\"shhpr82@gmail.com\"\n","\n","up_to_wetransfer(\"/content/szycra4.mp4\")\n","\n","# print(os.path.exists(\"file.txt\"))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"YmvNwxi8o1Ec","colab_type":"code","colab":{}},"source":["!sudo -H pip install --upgrade youtube-dl\n","# !youtube-dl https://www.youtube.com/watch?v=hbkZrOU1Zag --all-subs\n","# !youtube-dl --get-description https://www.youtube.com/watch?v=JUNzzpfeADI\n","# !youtube-dl https://www.youtube.com/watch?v=hbkZrOU1Zag --get-description\n","!youtube-dl https://soundcloud.com/armin-poyamanesh/ebi-adat\n","# !youtube-dl -h\n","####Support sites: https://github.com/ytdl-org/youtube-dl/tree/master/youtube_dl/extractor"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"RcJzADvrdyc3","colab_type":"code","colab":{}},"source":["try:\n","  from pytube import YouTube\n","except:\n","  !pip install pytube3\n","  from pytube import YouTube\n","link = str(input(\"Enter the link: \"))\n","yt = YouTube(link)\n","#Title of video\n","print(\"Title: \",yt.title)\n","#Number of views of video\n","print(\"Number of views: \",yt.views)\n","#Length of the video\n","print(\"Length of video: \",yt.length,\"seconds\")\n","#Description of video\n","print(\"Description: \",yt.description)\n","#Rating\n","print(\"Ratings: \",yt.rating)\n","ys = yt.streams.all()\n","for y2 in ys:\n","  print(y2)\n","indx = int(input('select the number to be downloaded: '))\n","ys[indx-1].download()\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Vnc2TDOd13op","colab_type":"code","colab":{}},"source":["try:\n","  import pafy\n","except:\n","  !sudo pip install pafy\n","  !sudo pip install youtube_dl\n","  import pafy\n","\n","url = \"https://www.youtube.com/watch?v=hbkZrOU1Zag\"\n","video = pafy.new(url)\n","print(video.title)\n","best = video.getbest()\n","print(best)\n","print(video.duration)\n","print(f'File Size in MB: {best.get_filesize()/10**6}')\n","print(\"*\"*10)\n","\n","def diir(Mdl):\n","  for i in dir(Mdl):\n","    try:\n","      print(str(i) + \"\\n\" + str(getattr(Mdl,i)))\n","    except:\n","      print(f\"{i} :  Error...\")\n","      continue\n","\n","# diir(best)\n","#print(video.description)\n","best.download(quiet=False)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"-g9pdXW_p6Xn","colab_type":"code","colab":{}},"source":["# !pip install download-youtube-subtitle\n","# !dl-youtube-cc hbkZrOU1Zag\n","\n","import download_youtube_subtitle.common as common\n","import download_youtube_subtitle.main as download_youtube_subtitle\n","\n","def diir(Mdl):\n","  for i in dir(Mdl):\n","    try:\n","      print(str(i) + \"\\n\" + str(getattr(Mdl,i)))\n","    except:\n","      print(f\"{i} :  Error...\")\n","      continue\n","\n","# htm = download_youtube_subtitle.get_data(\"https://www.youtube.com/watch?v=hbkZrOU1Zag\")\n","# diir(download_youtube_subtitle)\n","download_youtube_subtitle.main(\"hbkZrOU1Zag\", translation=\"fa\")\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"BmX7A4jRL3r3","colab_type":"code","colab":{}},"source":["# !git clone https://github.com/nstrydom2/anonfile-api.git\n","# !pip install wget\n","# !pip install functools\n","\n","#### Change anonfile-api folder to anonfile_api\n","\n","from anonfile_api.anonfile import anonfile\n","\n","# print(dir(anonfile))\n","anon = anonfile.AnonFile()\n","# file_info = anon.upload_file('/content/anonfile_api/requirements.txt')\n","file_info = anon.download_file(\"https://anonfiles.com/n061n3G1oc/requirements_txt\")\n","print(file_info)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZYBrFkHdlfR0","colab_type":"code","colab":{}},"source":["def file_size(fname):\n","        import os\n","        statinfo = os.stat(fname)\n","        #return statinfo.st_size\n","        print(\"File size in Kbytes: \",statinfo.st_size/1000)\n","        print(\"File size in Mbytes: \",statinfo.st_size/(10**6))\n","\n","file_size(\"Ebi.mp3\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"azBlj5i0YTJB","colab_type":"code","colab":{}},"source":["from random import randint\n","def chk_num(guess, correct_num):\n","\tif correct_num > guess:\n","\t\tprint(\"You have choosen a lower number.\")\n","\t\treturn False\n","\telif correct_num < guess:\n","\t\tprint(\"You have choosen an higher number.\")\n","\t\treturn False\n","\telif correct_num==guess:\n","\t\tprint(\"You successfully guessed the number: %d\" % (correct_num))\n","\t\treturn True\n","def run():\n","\tprint(\"Choose a number between 1 to 300\")\n","\tx=randint(1,300)\n","\ty=1\n","\twhile True:\n","\t\tz=int(input(\"Your Number:\\n\"))\n","\t\tif chk_num(z, x) == True:\n","\t\t\tprint(\"You guessed after %d times\" % (y))\n","\t\t\trpt = input(\"Do you want to repeat the game? (y or n)\")\n","\t\t\tif rpt.lower() == \"y\":\n","\t\t\t\trun()\n","\t\t\telse:\n","\t\t\t\tbreak\n","\t\tif y == 10:\n","\t\t\tprint(\"You LOST..\\n the Correct number is: %d\" % (x))\n","\t\t\tprint(\"You guessed %d times.\" % (y))\n","\t\t\trpt = input(\"Do you want to repeat the game? (y or n)\")\n","\t\t\tif rpt == \"y\":\n","\t\t\t\trun()\n","\t\t\telse:\n","\t\t\t\tbreak\n","\t\ty += 1\n","run()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"bUBaj-1jIZmC","colab_type":"code","colab":{}},"source":["def wr_to_file(file,things):\n","  with open(file,\"w\") as file1:\n","    file1.write(things)\n","\n","wr_to_file(\"s.txt\", \"Hiii\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"CZaRMHVsGAoO","colab_type":"code","colab":{}},"source":["import requests, re, unicodedata\n","headers = {'User-Agent': 'ECJHTTPHeaders/1.0 (iPad; iOS 12.4.6; Scale/2.00)'}\n","from bs4 import BeautifulSoup\n","from urllib.parse import urljoin\n","Filee = '/content/fara.htm'\n","with open(Filee, \"r\") as file1:\n","  soup=BeautifulSoup(file1)\n","\n","def url_join(url, filee, file_save):\n","  with open(file_save, \"w\") as file3:\n","    file3.close()\n","  with open(filee, \"r\") as file1:\n","    for line in file1:\n","      line=line.strip()\n","      with open(file_save, \"a\") as file2:\n","        file2.write(f'{str(urljoin(url, line))}\\n')\n","\n","\n","def pretty_soup(souup,filee):\n","  with open(filee, \"w\") as file1:\n","    file1.write(str(souup.prettify()))\n","\n","def fnd_all_soup(souup, thing, filee, subthing=None):\n","  if subthing==None:\n","    a_soup = souup.find_all(thing)\n","    with open(filee, \"w\") as file1:\n","      for i in a_soup:\n","        file1.write(str(i.encode().decode('unicode-escape').encode('latin1').decode('utf-8')) + \"\\n\")\n","  else:\n","    a_soup = souup.find_all(thing)\n","    with open(filee, \"w\") as file1:\n","      for i in a_soup:\n","        print(i.get(subthing))\n","        file1.write(str(i.get(subthing)) + \"\\n\")\n","\n","def soup_get_text(souup, filee):\n","  with open(filee, \"w\") as file1:\n","    file1.write(str(soup.get_text()))\n","\n","def fnd_all_links(souup, strng):\n","  lnks_lst=[]\n","  a_soup=souup.find_all('a', href=True)\n","  with open('/content/drive/My Drive/all_links.txt', 'w') as file1:\n","    for i in a_soup:\n","      link=str(i['href'])\n","      if strng in link and link not in lnks_lst:\n","        print(link)\n","        lnks_lst.append(link)\n","        file1.write(link)\n","\n","def diir(Mdl, filee):\n","  with open(filee, \"w\") as file1:\n","    for i in dir(Mdl):\n","      try:\n","        file1.write(str(i) + \":::\\n\" + str(getattr(Mdl,i)) + \"\\n*********\\n\")\n","      except:\n","        file1.write(f\"{i} :  Error...\" + \"\\n\")\n","        continue\n","\n","def specify_soup(souup, strng, filee):\n","  with open(filee, \"w\") as file1:\n","    for i in souup.find_all(True):\n","      if (i.string != None) and (strng in i.string):\n","        file1.write(f\"{i}\\n{i.string}\\n*********\\n\")\n","\n","def spcfy_srch(souup, parnt, child, strng):\n","  with open('/content/drive/My Drive/spcfy_srch.txt', 'w') as file1:\n","    a_soup= souup.find_all(parnt)\n","    for i in a_soup:\n","      spc = str(i.get(child))\n","      if strng in spc:\n","        print(i)\n","        print(spc)\n","        file1.write(f'{i}\\n{spc}\\n\\n*********\\n\\n')\n","\n","def get_str_by_id_soup(souup, idd):\n","  for i in souup.find_all(id=idd):\n","    print(i)\n","    print(i.string)\n","\n","def fnd_input_value(souup,filee):\n","  name_list=[]\n","  val_list=[]\n","  a_soup = souup.find_all(\"input\")\n","  with open(filee, \"w\") as file1:\n","    for i in a_soup:\n","      name_list.append(i.get(\"name\"))\n","      val_list.append(i.get(\"value\"))\n","    # print(name_list)\n","    # print(val_list)\n","    inp_dict={}\n","    for i in range(len(name_list)):\n","      inp_dict[name_list[i]] = val_list[i]\n","\n","    print(inp_dict)\n","    file1.write(f'{str(inp_dict)}\\nnames:\\n{name_list}\\nvalues:\\n{val_list}')\n","\n","def fnd_by_attrs(souup, parnt, child, strng):\n","  with open('fnd_by_attrs.txt' , 'w') as file1:\n","    fnd = str(souup.find_all(parnt, attrs={child:strng}))\n","    file1.write(fnd)\n","    print(fnd)\n","\n","\n","# diir(soup, \"dir_soup.txt\")\n","# print(soup.find_all(\"a\")[13].get(\"href\"))\n","# print(dir(soup.find_all(text='Programming and related technical career opportunities')))\n","# print(soup.find_all(text=re.compile('more.html')))\n","# print(soup.find_all(\"code\")[1].string)\n","# print(soup.find_all(True, attrs={'class':\"fc-light\"})[0].string)\n","# print(soup.find_all(href=re.compile('.rar'))[0].get('href'))\n","# print(soup(attrs={'src':re.compile('.mp4')}))\n","print(soup.select('a[href]'))\n","# fnd_all_soup(soup, 'link', \"fnd.txt\", subthing=\"href\" )\n","# fnd_all_soup(soup, \"span\", 'span.txt' )\n","# fnd_all_soup(soup, 'a', \"fnd.txt\", \"href\")\n","# fnd_all_soup(soup, 'svg', \"svg.txt\", 'class')\n","#fnd_all_soup(soup, 'title', \"fnd.txt\")\n","# fnd_all_soup(soup, 'form', \"forms.txt\")\n","# fnd_all_soup(soup, 'input', \"inputs.txt\")\n","# url_join(\"https://www.manototv.com/show/1021\", \"fnd.txt\", \"links.txt\")\n","# soup_get_text(soup, \"gtx.txt\")\n","# specify_soup(soup, \"34.69\", \"34.txt\")\n","# get_str_by_id_soup(soup, \"search\")\n","# pretty_soup(soup, \"pretty_soup.htm\")\n","# fnd_input_value(soup, \"inp.txt\")\n","# spcfy_srch(soup, 'div', 'class', 'js-accepted-answer-indicator')\n","# fnd_all_links(soup, 'mp3')\n","#fnd_by_attrs(soup, 'div', 'class', 'js-accepted-answer-indicator grid--cell fc-green-500 ta-center py4')\n","# help(re)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"9Q1FsT5bgbIZ","colab_type":"code","colab":{}},"source":["# !pip install proxybroker\n","try:\n","  import asyncio\n","  from proxybroker import Broker\n","except:\n","  !pip install proxybroker\n","  import asyncio\n","  from proxybroker import Broker\n","\n","async def save(proxies, filename):\n","    \"\"\"Save proxies to a file.\"\"\"\n","    with open(filename, 'w') as f:\n","        while True:\n","            proxy = await proxies.get()\n","            if proxy is None:\n","                break\n","            proto = 'https' if 'HTTPS' in proxy.types else 'http'\n","            row = '%s:%d\\n' % (proxy.host, proxy.port)\n","            f.write(row)\n","\n","proxies = asyncio.Queue()\n","broker = Broker(proxies)\n","tasks = asyncio.gather(broker.find(types=['HTTPS'], limit=30),\n","                        save(proxies, filename='proxies.txt'))\n","loop = asyncio.get_event_loop()\n","loop.run_until_complete(tasks)\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"HQmzcb7nFssG","colab_type":"code","colab":{}},"source":["import os, sys\n","import requests\n","import re\n","headers = {'User-Agent': 'ECJHTTPHeaders/1.0 (iPad; iOS 12.4.6; Scale/2.00)'}\n","\n","\n","def proxy_via_os(proxy):\n","  proxies = {\"https\":\"https://%s\" % (proxy) , 'http':'http://%s' % (proxy)}\n","  try:\n","\n","    # os.environ[\"HTTPS_PROXY\"] = \"https://%s\" % (proxy)\n","    print(proxy)\n","    response = requests.get('https://faradars.org/courses/fvcvl9510-earthquake-engineering', headers=headers, proxies=proxies, timeout=10)\n","    print(str(response))\n","    if '200' in str(response):\n","        with open('fara.htm', 'w') as file1:\n","          file1.write(str(response.content))\n","        sys.exit()\n","      # with open('/content/drive/My Drive/ok.txt', 'a') as file22:\n","      #   file22.write(f'{proxy}\\n')\n","  \n","\n","    # print(response.content)\n","    # fnd = re.findall('.*\\d[.].*\\d[.].*\\d[.].*\\d', response.content)\n","    # fnd2 = re.findall('(?<=<td>Country</td><td>)(.*)(?=</td)', response.content)\n","    # fnd2 = re.findall('(?<=Country:</th><td style=\"font-size:14px;\">)(.*)(?=</td>)', str(response.content))\n","    \n","\n","    # print(fnd[0])\n","    # print(fnd2)\n","\n","  except KeyboardInterrupt:\n","    sys.exit()\n","  except Exception as ex:\n","    print(ex)\n","  \n","def read_file():\n","\twith open('/content/drive/My Drive/ok.txt','r') as file1:\n","\t\tfor line in file1:\n","\t\t\tproxy_lst = line.strip()\n","\t\t\tproxy_via_os(proxy_lst)\n","\n","read_file()\n","# proxy_via_os(\"185.208.172.252:3128\")\n","# proxy_via_os(\"54.38.218.214:6582\")\n","# proxy_via_os(\"185.83.197.228:8080\")\n","# proxy_via_os(\"81.201.60.130:80\")\n","# proxy_via_os(\"200.106.55.125:80\")\n","# proxy_via_os(\"13.75.114.68:25222\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"mgH8XF2nVVHQ","colab_type":"code","colab":{}},"source":["import requests\n","\n","session = requests.Session()\n","session.trust_env = False"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"pKLqU1xJl5Po","colab_type":"code","colab":{}},"source":["import requests,os\n","# os.environ['HTTPS_PROXY'] = \"https://193.148.67.106:8080\"\n","headers ={\"Host\": \"www.shatelland.com\",\n","\"User-Agent\": \"Mozilla/5.0 (Windows NT 6.2; Win64; x64; rv:67.0) Gecko/20100101 Firefox/67.0\",\n","\"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8\",\n","\"Accept-Language\": \"en-US,en;q=0.5\",\n","\"Accept-Encoding\": \"gzip, deflate, br\",\n","\"Referer\": \"https://www.shatelland.com/upload\",\n","\"Content-Type\": \"application/x-www-form-urlencoded\",\n","\"Content-Length\": \"99\",\n","\"Connection\": \"keep-alive\",\n","\"Cookie\": \"_ga=GA1.2.1132584396.1584450241; analytics_campaign={%22source%22:%22direct%22%2C%22medium%22:null}; analytics_token=d0c17e92-1f24-1d2e-3a52-c3293f1ad143; authv4=220017CB5CE0E1B43FC33DF9F573847534B01357A6903E76A9DC0E95B392EA00DDA43B834F035D3A57B5691E2ACDEFBC239FEC486B6496287BFD3B134BEC1258EB029FCF1DF4071D0A5EB06C0E503A8A9207E332A9D4A5A9E4F96942D220EAB9D06776034DACD7314AE07A63DDDE1654FC396010760BF229AECEFA1475AA1AB7FC0E0F632D71636C9927E4B2E010AEDDC7DCA43B2EFC178F54B877E38DF3C631F71CD520D0BFB93E9E003531014D6CCB234777339CC5530EF72987599DB97EA841FA020785E5DC29FBD07ACCCCD0BC73DBDBB11D54ED1B9272C0CDF79C70E2E9574A725CADDA16773B5775B669471A6D\",\n","\"Upgrade-Insecure-Requests\": \"1\",\n","\"Pragma\": \"no-cache\",\n","\"Cache-Control\": \"no-cache\",\n","}\n","\n","URL = 'https://dl4.shatelland.com/api/LeechManager/currentuser'\n","files = {'files': open('/content/drive/My Drive/Colab Notebooks/Image to pencil.ipynb','rb')}\n","payload = {'Email': 'shhpr82@gmail.com', 'Password': \"09129332660\", 'Remember': 'true', 'Item1.Remember': 'false', 'redirectTo': '~/Upload/Index'}\n","\n","session = requests.session()\n","r = requests.post(URL, data=payload,files=files, headers=headers)\n","# print(r.cookies)\n","print(r.content.decode())\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"W-2yzTSSO-a_","colab_type":"code","colab":{}},"source":["try:\n","  import findimports\n","except:\n","  !pip install findimports\n","  import findimports\n","# !pip3 install findimports\n","# import findimports\n","# help(findimports)\n","# for i in dir(findimports):\n","#   break\n","#   print(\"********\")\n","#   print(i)\n","#   print(getattr(findimports,i))\n","\n","# findimports.find_imports(\"/usr/local/lib/python3.6/dist-packages/pafy/__init__.py\")\n","findimports.find_imports(\"/usr/local/lib/python3.6/dist-packages/pytube/request.py\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"U1Gdw_nFp0RJ","colab_type":"code","colab":{}},"source":["import smtplib\n","from email.mime.multipart import MIMEMultipart\n","from email.mime.text import MIMEText\n","from email.mime.base import MIMEBase\n","from email import encoders\n","mail_content = '''Hello,\n","This is a test mail.\n","In this mail we are sending some attachments.\n","The mail is sent using Python SMTP library.\n","Thank You\n","'''\n","#The mail addresses and password\n","sender_address = 'jigaarr24@gmail.com'\n","sender_pass = 'hwtdwmygavinyrcs'\n","receiver_address = 'srhpr80@gmail.com'\n","#Setup the MIME\n","message = MIMEMultipart()\n","message['From'] = sender_address\n","message['To'] = receiver_address\n","message['Subject'] = '2.ipynb'\n","#The subject line\n","#The body and the attachments for the mail\n","message.attach(MIMEText(mail_content, 'plain'))\n","attach_file_name = '/content/drive/My Drive/Colab Notebooks/2.ipynb'\n","attach_file = open(attach_file_name, 'rb') # Open the file as binary mode\n","payload = MIMEBase('application', 'octate-stream')\n","payload.set_payload((attach_file).read())\n","encoders.encode_base64(payload) #encode the attachment\n","#add payload header with filename\n","payload.add_header('Content-Decomposition', 'attachment', filename=attach_file_name)\n","message.attach(payload)\n","#Create SMTP session for sending the mail\n","session = smtplib.SMTP('smtp.gmail.com', 587) #use gmail with port\n","session.starttls() #enable security\n","session.login(sender_address, sender_pass) #login with mail_id and password\n","text = message.as_string()\n","session.sendmail(sender_address, receiver_address, text)\n","session.quit()\n","print('Mail Sent')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"d8E-PjVCVqH5","colab_type":"code","colab":{}},"source":["#################\n","#################\n","\n","try:\n","  from selenium import webdriver\n","  from selenium.webdriver.common.keys import Keys\n","  from selenium.webdriver.common.action_chains import ActionChains\n","  from selenium.webdriver.support import ui\n","  from selenium.webdriver.support import expected_conditions as EC\n","  from selenium.webdriver.support.ui import WebDriverWait\n","  from selenium.webdriver.common.by import By\n","except:\n","    !apt-get update\n","    !apt install chromium-chromedriver\n","    !cp /usr/lib/chromium-browser/chromedriver /usr/bin\n","    !pip install selenium\n","    from selenium import webdriver\n","    from selenium.webdriver.common.keys import Keys\n","    from selenium.webdriver.common.action_chains import ActionChains\n","    from selenium.webdriver.support import ui\n","    from selenium.webdriver.support import expected_conditions as EC\n","    from selenium.webdriver.support.ui import WebDriverWait\n","    from selenium.webdriver.common.by import By\n","# set options to be headless, ..\n","import pickle, re, time\n","from IPython.display import clear_output\n","from bs4 import BeautifulSoup\n","\n","\n","def xpath_soup(element):\n","    # type: (typing.Union[bs4.element.Tag, bs4.element.NavigableString]) -> str\n","    components = []\n","    child = element if element.name else element.parent\n","    for parent in child.parents:  # type: bs4.element.Tag\n","        siblings = parent.find_all(child.name, recursive=False)\n","        components.append(\n","            child.name if 1 == len(siblings) else '%s[%d]' % (\n","                child.name,\n","                next(i for i, s in enumerate(siblings, 1) if s is child)\n","                )\n","            )\n","        child = parent\n","    components.reverse()\n","    return '/%s' % '/'.join(components)\n","\n","def save_cookie(driver,filee=\"cookies.pkl\"):\n","  pickle.dump(driver.get_cookies() , open(filee,\"wb\"))\n","\n","def load_cookie(driver, filee=\"cookies.pkl\"):\n","  cookies = pickle.load(open(filee, \"rb\"))\n","  for cookie in cookies:\n","    driver.add_cookie(cookie)\n","\n","def diir(Mdl, filee):\n","  with open(filee, \"w\") as file1:\n","    for i in dir(Mdl):\n","      try:\n","        file1.write(str(i) + \":::\\n\" + str(getattr(Mdl,i)) + \"\\n*********\\n\")\n","      except:\n","        file1.write(f\"{i} :  Error...\" + \"\\n\")\n","        continue\n","\n","def sndkeys_or_click(element):\n","  s_or_c = str(input('cls_and_sendkeys(cs) or sendkeys(s) or click(c) or Text(t) or Cls_sks_Enter(cse) or Submit(sb) :\\n'))\n","  if s_or_c == 's':\n","    things = str(input('sendkeys: '))\n","    element.send_keys(things)\n","  elif s_or_c == 'cs':\n","    element.clear()\n","    things = str(input('sendkeys: '))\n","    element.send_keys(things)\n","  elif s_or_c == 'c':\n","    element.click()\n","  elif s_or_c == 't':\n","    print(element.text)\n","    # print(element.getText())\n","  elif s_or_c == 'cse':\n","    element.clear()\n","    things = str(input('sendkeys: '))\n","    element.send_keys(things)\n","    element.send_keys(Keys.RETURN)\n","  elif s_or_c == 'sb':\n","    element.submit()\n","\n","def Beauty(driver):\n","  soup=BeautifulSoup(driver.page_source, \"html.parser\")\n","  with open(\"sele.htm\", \"w\") as file1:\n","    file1.write(str(soup.prettify()))\n","\n","\n","# PROXY = \"200.106.55.125:80\"\n","# PROXY = \"185.83.197.228:8080\"\n","# PROXY = \"81.201.60.130:80\"\n","# PROXY = \"54.38.218.208:6582\"\n","options = webdriver.ChromeOptions()\n","options.add_argument('--headless')\n","options.add_argument('--no-sandbox')\n","options.add_argument('--disable-dev-shm-usage')\n","options.add_argument(\"--enable-javascript\")\n","# options.add_argument('--proxy-server=%s' % PROXY)\n","wd = webdriver.Chrome('chromedriver',options=options)\n","wd.maximize_window()\n","\n","url= str(input('URL: '))\n","wd.get(url)\n","\n","while True:\n","  try:\n","    sth1=str(input('Screenshot(s) or Refresh(r) or Url(u) or iFrame(i) or Inner HTML(ih) or Outer HTML(oh) or Title(t) or Body_html(bh) or JS Code(js) or Save_Cookie(sc) or load_cookie(lc) or Clear_Output(co) or Doing sth:\\n '))\n","    if sth1 == 's':\n","      wd.save_screenshot('screen.png')\n","      Beauty(wd)\n","    elif sth1 == 'r':\n","      wd.refresh()\n","    elif sth1 == 'u':\n","      url = str(input('URL: '))\n","      wd.get(url)\n","    elif sth1 == 'i':\n","      if_str = str(input('frame:'))\n","      iframe = wd.find_element_by_xpath(if_str)\n","      wd.switch_to.frame(iframe)\n","    elif sth1 == 'ih':\n","      html2 = wd.execute_script(\"return document.documentElement.innerHTML;\")\n","      soup=BeautifulSoup(html2, \"html.parser\")\n","      with open(\"sele.htm\", \"w\") as file1:\n","        file1.write(str(soup.prettify()))\n","    elif sth1 == 'oh':\n","      html3 = wd.execute_script(\"return document.documentElement.outerHTML;\")\n","      soup=BeautifulSoup(html3, \"html.parser\")\n","      with open(\"sele.htm\", \"w\") as file1:\n","        file1.write(str(soup.prettify()))\n","    elif sth1 == 'bh':\n","      html3 = wd.execute_script(\"return document.body.innerHTML;\")\n","      soup=BeautifulSoup(html3, \"html.parser\")\n","      with open(\"sele.htm\", \"w\") as file1:\n","        file1.write(str(soup.prettify()))\n","    elif sth1 == 'js':\n","      js_str = str(input('javascript:' ))\n","      # window.scrollTo(0, 0);\n","      wd.execute_script(js_str)\n","    elif sth1 == 'sc':\n","      save_cookie(wd)\n","    elif sth1 == 'lc':\n","      lc_str = str(input('Cookie file: '))\n","      load_cookie(wd,lc_str)\n","    elif sth1 == 'co':\n","      clear_output()\n","      time.sleep(1)\n","    elif sth1 == 'b':\n","      break\n","    elif sth1 == 't':\n","      print(wd.title)\n","    else:\n","      str2=str(input('class(c) or id(i) or links(l) or name(n) or Xpath(x) or Css(css) or All_ids(ai) or All_xpath(ax):\\n'))\n","      if str2=='c':\n","        c_str = str(input('class name: '))\n","        elem = wd.find_element_by_class_name(c_str)\n","        sndkeys_or_click(elem)\n","      elif str2=='i':\n","        i_str = str(input('id: '))\n","        elem = wd.find_element_by_id(i_str)\n","        # print(dir(elem))\n","        sndkeys_or_click(elem)\n","      elif str2=='l':\n","        l_str = str(input('link name: '))\n","        elem = wd.find_element_by_partial_link_text(l_str)\n","        sndkeys_or_click(elem)\n","      elif str2=='n':\n","        n_str = str(input('name: '))\n","        elem = wd.find_element_by_name(n_str)\n","        sndkeys_or_click(elem)\n","      elif str2=='x':\n","        try:\n","          x_str = str(input('XPATH: '))\n","          # elem = wd.find_element_by_xpath(x_str)\n","          wait = WebDriverWait(wd, 10)\n","          elem = wait.until(EC.presence_of_element_located((By.XPATH,x_str)))\n","          sndkeys_or_click(elem)\n","        except Exception as ex:\n","          print(ex)\n","      elif str2=='css':\n","        css_str = str(input('CSS: '))\n","        # elem = wd.find_element_by_xpath(x_str)\n","        wait = WebDriverWait(wd, 10)\n","        elem = wait.until(EC.presence_of_element_located((By.CSS_SELECTOR,css_str)))\n","        sndkeys_or_click(elem)\n","      elif str2 == 'ai':\n","        # ids = \n","        ids = wd.find_elements_by_css_selector('*')\n","        for ii in ids:\n","            # print(xpath_soup(ii))\n","            # print(dir(ii))\n","            break\n","            # print(ii.get_attribute('id'))\n","      elif str2 == 'ax':\n","        xx = wd.find_elements_by_xpath('//*')\n","        print(len(xx))\n","        int_ax = int(input('len: '))\n","        cnt=0\n","        for x in xx:\n","          cnt += 1\n","          if x.get_attribute('id') or x.get_attribute('name') or x.get_attribute('type'):\n","            print(f\"id: {x.get_attribute('id')}\\n value: {x.get_attribute('value')}\\n type: {x.get_attribute('type')}\\n name: {x.get_attribute('name')}\\n class: {x.get_attribute('class')}\\n\")\n","            # print(x.text)\n","          if cnt >= int_ax :\n","            break\n","  except KeyboardInterrupt as k:\n","    print(k)\n","  except ConnectionRefusedError as cre:\n","    wd.get(url)\n","\n","wd.close()\n","\n","# https://www.speedtest.net/\n","# //*[@id=\"container\"]/div/div[3]/div/div/div/div[2]/div[3]/div[1]/a\n","# window.scrollTo(0, 0);"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"dRPtOre4d3Uc","colab_type":"code","colab":{}},"source":["# !/usr/bin/python2.7\n","#discover urls in the domain by extract the href link in the content and crawl recursively to get all urls\n","import requests, sys\n","from bs4 import BeautifulSoup\n","import re\n","from urllib.parse import urljoin\n","\n","#target_url = \"https://stackoverflow.com/questions/tagged/python?tab=newest&page=99889&pagesize=15\"\n","target_url = 'https://niloomusic6.ir'\n","target_links = []\n","headers = {'User-Agent': 'ECJHTTPHeaders/1.0 (iPad; iOS 12.4.6; Scale/2.00)'}\n","proxy = '13.75.114.68:25222'\n","proxies = {\"https\":\"https://%s\" % (proxy) , 'http':'http://%s' % (proxy)}\n","mp3_lst=[]\n","\n","def extract_links_from(url):\n","  global mp3_lst\n","  response = requests.get(url, proxies=proxies, headers=headers, timeout=20)\n","  # response = requests.get(url, headers=headers)\n","  str1=re.findall('<title>(.*?)</title>', str(response.content))\n","  #str2 = re.findall('(\"js-accepted-answer-indicator grid--cell fc-green-500 ta-center py4\")', str(response.content))\n","  # str2=re.findall('([.a-z0-9_-]+@[a-z.]+.[a-z]:[\\S]+)', str(response.content))\n","  str2=re.findall(r'(?<=href=\")(\\S+)(?=.mp3\")', str(response.content))\n","\n","  if len(str1) > 0 and len(str2)>=1:\n","      str3 = str1[0].encode().decode('unicode-escape').encode('latin1').decode('utf-8')\n","      #print(str2)\n","      #sys.exit()\n","      with open('/content/drive/My Drive/nil.txt', 'a') as file1:\n","        file1.write(f'{url}\\n{str3}\\n')\n","        for mp3 in str2:\n","          if mp3 not in mp3_lst:\n","            mp3_lst.append(mp3)\n","            file1.write(f'{mp3}.mp3\\n\\n')\n","  return re.findall('(?:href=\")(.*?)\"', str(response.content))\n","\n","\n","def crawl(url):\n","  try:\n","    href_links = extract_links_from(url)\n","    for link in href_links:\n","      link = urljoin(url,link)\n","\n","      # print(link)\n","      if 'team@stackexchange.com' in link or \"java\" in link:\n","        continue\n","      if \"#\" in link:\t# # refer+s to original page so avoid duplicate page again and again\n","        link = link.split(\"#\")[0]\n","      #if \"?\" in link:\t# # refer+s to original page so avoid duplicate page again and again\n","      #  link = link.split(\"?\")[0]\n","      if str(link)[-1] == '/':\n","        link=link[:-1]\n","      if 'niloomusic' in link and link not in target_links: \n","        target_links.append(link)\n","        #str1=str(input(f'{link}\\n'))\n","        str1='A'\n","        if str1=='A':\n","          #print(link)\n","          crawl(link) #recursively crawling\n","        elif str1=='d':\n","          print('Downloding...')\n","  except Exception as ex:\n","    print(ex)\n","    # pass\n","crawl(target_url)\n","\n","# https://stackoverflow.com/questions/tagged/python?tab=newest&page=99889&pagesize=15\n","# iconCheckmarkLg\n","# \"js-accepted-answer-indicator grid--cell fc-green-500 ta-center py4\""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"jEYCrkS03-du","colab_type":"code","colab":{}},"source":["from bs4 import BeautifulSoup\n","import requests, re, sys, time\n","from IPython.display import clear_output\n","from urllib.parse import urljoin\n","\n","headers = {'User-Agent': 'ECJHTTPHeaders/1.0 (iPad; iOS 12.4.6; Scale/2.00)'}\n","proxy = '18.218.173.238:3838'\n","proxies = {\"https\":\"https://%s\" % (proxy) , 'http':'http://%s' % (proxy)}\n","all_links=[]\n","target_lnks=[]\n","Url = 'https://niloomusic6.ir/'\n","\n","# a_soup=souup.find_all('a', href=True)\n","\n","def purpose(souup, url):\n","  # b_soup = souup.find_all('div', attrs={'class':'js-accepted-answer-indicator grid--cell fc-green-500 ta-center py4'})\n","  b_soup = souup.find_all('a', href=True)\n","  b_chk=False\n","  b4 = \"\"\n","  for bb in b_soup:\n","    b3 = bb.get('href')\n","    if '.mp3' in b3 and b3 not in b4:\n","      # print(\"mp3\")\n","      # print(b3)\n","      b_chk = True\n","      b4= b4 + f'{b3}\\n'\n","  if b_chk:\n","    print(url)\n","    with open('/content/drive/My Drive/bs4_crawl.txt', 'a') as file1:\n","      file1.write(f'{souup.title.string}\\n{url}\\n\\n{b4}\\n\\n******\\n\\n')\n","  # c_soup = souup.find_all('a', attrs={'class':'post-tag'})\n","  # c_chk=False\n","  # for cc in c_soup:\n","  #   if cc.string == 'python':\n","  #     c_chk=True\n","\n","  # if (c_chk) and (('none' not in b_soup) and len(b_soup)>0):\n","  #   with open('/content/drive/My Drive/bs4_crawl.txt', 'a') as file1:\n","  #     file1.write(f'{souup.title.string}\\n{url}\\n\\n')\n","  #     print(souup.title.string)\n","  #     print(url)  \n","  return b_soup\n","\n","def chk(url):\n","  try:\n","    global all_links, target_lnks\n","    response = requests.get(url, proxies=proxies, headers=headers)\n","    # response = requests.get(url , headers=headers)\n","    # print(response.status_code)\n","    if response.status_code == 200:\n","      # print(\"OK\")\n","      target_lnks.append(url)\n","      soup = BeautifulSoup(response.content)\n","      a_soup=soup.find_all('a', href=True)\n","      for link in a_soup:\n","        lnk_txt = link.get(\"href\")\n","        link2 = urljoin(str(url),str(lnk_txt))\n","        if \"#\" in link2:\n","          continue\n","        if ('niloomusic' in link2) and (link2 not in all_links):\n","          # print(link2)\n","          all_links.append(link2)\n","      purpose(soup, url)\n","  except KeyboardInterrupt:\n","    # print(all_links)\n","    # print(\"target_lnks:::::::\")\n","    # print(target_lnks)\n","    sys.exit()\n","  except Exception as ex:\n","    if '@' in url:\n","      target_lnks.append(url)\n","    print(ex)\n","    #pass\n","\n","all_links.append(Url)\n","cnt=0\n","while True:\n","  for i in all_links:\n","    cnt += 1\n","    if cnt>20:\n","      clear_output(wait=True)\n","      cnt=0\n","    if i not in target_lnks:\n","      # print(i)\n","      chk(i)\n","  time.sleep(5)"],"execution_count":null,"outputs":[]}]}